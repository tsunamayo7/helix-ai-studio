I have been spending the last year building an app to solve a problem that kept annoying me: I use Claude for planning and analysis, but I have a perfectly capable local GPU sitting there that could handle the execution work. Copy-pasting between a cloud chat window and a local Ollama terminal felt broken. So I built something that connects them into a single pipeline.

## What it does

**Helix AI Studio** is a PyQt6 desktop app (MIT licensed) that runs a 3+1 Phase orchestration pipeline. The core idea: Claude does the thinking, your local Ollama models do the work, Claude checks the result.

```
                      Your Prompt
                          |
                          v
              +------------------------+
              |   PHASE 1 (Cloud)      |
              |   Claude / GPT / Gemini|
              |   - Task decomposition |
              |   - Acceptance criteria |
              |   - Per-model instruct.|
              +------------------------+
                          |
            +-------------+-------------+
            |             |             |
            v             v             v
  +----------+  +----------+  +----------+
  | Coding   |  | Research |  | Reasoning|  ... up to 5 categories
  | devstral |  | qwen3    |  | qwq      |
  | :24b     |  | :32b     |  | :32b     |
  +----------+  +----------+  +----------+
            |             |             |
            +-------------+-------------+
                          |
                          v
              +------------------------+
              |   PHASE 3 (Cloud)      |
              |   Claude validates     |
              |   PASS/FAIL per crit.  |
              |   Integrated response  |
              +------------------------+
                          |
                          v
              +------------------------+
              |   PHASE 4 (Optional)   |
              |   Apply file changes   |
              +------------------------+
```

## How Phase 2 works (the local part)

Phase 2 dispatches to up to 5 specialist categories on Ollama:

| Category | Example Models | Role |
|---|---|---|
| Coding | devstral:24b, qwen2.5-coder:32b | Implementation, refactoring, debugging |
| Research | qwen3:32b, gemma3:27b | Fact-checking, analysis |
| Reasoning | qwq:32b, deepseek-r1:32b | Logic validation, edge cases |
| Translation | qwen3:32b | i18n, documentation |
| Vision | llava:34b, gemma3:27b | Image understanding |

Each category is independently togglable. The **SequentialExecutor** manages VRAM -- it follows a strict load-run-unload cycle, so you can run five different 32B models on a single GPU (24GB VRAM). It just runs them one at a time.

**Phase 2 is 100% local. Zero API cost. Your data never leaves your machine.**

## The cost math

A typical pipeline run makes exactly 2 cloud API calls:
- Phase 1: Claude plans (~$0.02-0.08 depending on model/context)
- Phase 3: Claude validates (~$0.02-0.08)

Phase 2 -- the heavy lifting -- is free. Your local GPU handles all execution work.

So Claude does roughly 20% of the work (planning + validation), and your free local models do 80% (execution). For a project where you might run 20-30 pipeline executions per day, that is a significant difference versus sending everything to a cloud API.

## What makes this different from LangGraph / CrewAI / AutoGen?

Those are frameworks -- you write Python code to define agents, chains, and workflows. Helix is a GUI app. No YAML, no agent code, no framework boilerplate. You type a prompt, pick your models in a dropdown, and the pipeline runs. Settings panels instead of config files.

## What makes this different from Open WebUI / Ollama WebUI?

Those are single-model chat interfaces. Helix routes your prompt through multiple models with different specializations and synthesizes the results via explicit acceptance criteria. It is not a chat UI -- it is an orchestration layer.

## Beyond the pipeline

- **cloudAI tab**: Direct chat with Claude/GPT/Gemini (streaming, file attachments, adaptive thinking)
- **localAI tab**: Direct Ollama chat with tool use (filesystem, code execution)
- **4-layer memory**: Thread / Episodic (RAPTOR summaries) / Semantic (knowledge graph) / Procedural (learned patterns)
- **BIBLE-first documentation**: Auto-injects your project spec into Phase 1/3 context
- **RAG pipeline**: Document chunking + vector search
- **Web UI**: Built-in React + FastAPI web interface accessible from your phone on the same network
- **MCP support**: Model Context Protocol servers for filesystem, git, web search

## Tech stack

```
Desktop GUI:     PyQt6
Web Backend:     FastAPI + Uvicorn + WebSocket
Web Frontend:    React + Tailwind CSS
Local LLMs:      Ollama (httpx streaming, sequential executor)
Cloud AI:        Claude CLI / Anthropic API / OpenAI API / Gemini API
Database:        SQLite (WAL mode, shared Desktop + Web)
i18n:            Japanese + English
License:         MIT
```

No telemetry. No analytics. No phoning home. Discord webhook notifications are opt-in and only go to YOUR webhook.

## Links

- **GitHub**: https://github.com/tsunamayo7/helix-ai-studio
- **Setup Guide**: https://github.com/tsunamayo7/helix-ai-studio/blob/main/SETUP_GUIDE.md

v11.9.4 "Helix Pilot", Windows, Python 3.12+. MIT licensed.

**New in v11.9.4**: Helix Pilot -- a GUI automation tool that uses local Vision LLMs (Mistral Small 3.2, Gemma3:27b) for autonomous desktop operation. The local model sees your screen, plans multi-step actions, and executes them with built-in safety validation. Context consumption reduced by 90%+ compared to raw screenshot-based automation.

Happy to answer any questions about the architecture, local model performance, or VRAM management. Stars welcome!