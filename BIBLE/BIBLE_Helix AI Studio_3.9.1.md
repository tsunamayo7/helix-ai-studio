# Helix AI Studio - Project Bible (包括的マスター設計書)

**バージョン**: 3.9.1
**アプリケーションバージョン**: 3.9.1 "Helix AI Studio - LLMmixバグ修正・UI改善"
**作成日**: 2026-02-02
**最終更新**: 2026-02-02
**目的**: プロジェクトの全容・経緯・設計思想を1ファイルで完全に把握するための聖典

---

## v3.9.1 更新履歴 (2026-02-02)

### 重大バグ修正 & UI改善アップデート

**概要**:
v3.9.1 は LLMmix タブのモデル一覧表示バグを修正し、役割別モデル割り当てのUIを改善。
シミュレーションモード表示を追加し、ユーザーへの情報提供を強化。

**修正内容**:

| 問題 | 原因 | 対策 |
|------|------|------|
| 「接続成功（13モデル）」と表示されるがモデル一覧テーブルが空 | `_test_ollama_connection()` がテーブル更新を行わない | 接続テスト成功後に `_populate_ollama_models_table()` を自動呼び出し |
| 役割コンボボックスにOllamaモデルが追加されない | 上記と同様 | 共通メソッドで一括更新 |
| プロバイダが不明瞭 | 単純な表記 | `[Claude] Opus 4.5`、`[Ollama] llama3.2:8b` 形式に統一 |
| シミュレーション/実呼び出しの区別が不明 | 表示なし | 警告ラベル「シミュレーションモード」を追加 |

---

## タブ構成 (v3.9.1)

### タブ構成 (4タブ)

| # | タブ名 | サブタブ | 説明 |
|---|--------|----------|------|
| 1 | 🤖 Claude | チャット / 設定 | AIチャット＆設定統合 |
| 2 | 🔀 LLMmix | チャット / 設定 | マルチLLMオーケストレーション |
| 3 | 📝 チャット作成 | - | チャット原稿の作成・編集 |
| 4 | ⚙️ 一般設定 | - | アプリ全体の設定 |

---

## LLMmix タブ詳細 (v3.9.1 更新)

### モデル一覧表示の仕様

**接続テスト → モデル一覧自動更新フロー**:

```
[接続テスト] ボタンクリック
    ↓
_test_ollama_connection() 実行
    ↓
ollama.Client.list() でモデル取得
    ↓
成功時: _populate_ollama_models_table() 自動呼び出し
    ↓
1. テーブル (QTableWidget) にモデル名・サイズ・テストボタンを追加
2. 役割コンボボックスに [Ollama] モデル名 形式で追加
3. ステータスラベル更新: 「✅ 13 モデルを取得 (接続先: http://localhost:11434)」
```

**デバッグログ出力** (logger.info):
- 取得したモデル数
- テーブル反映後の rowCount
- モデル名リスト

### 7役割モデル割り当ての仕様 (v3.9.1 更新)

**ドロップダウン選択の統合リスト**:

```
── Claude ──
[Claude] Opus 4.5 (最高性能)
[Claude] Sonnet 4.5 (推奨)
[Claude] Haiku 4.5 (高速)
── Ollama (ローカル) ──
[Ollama] llama3.2:8b
[Ollama] qwen3-coder
[Ollama] ...（動的に追加）
```

**テストボタンの動作**:
- Ollamaモデル: 簡易プロンプト送信 → 応答表示
- Claudeモデル: テストスキップ（APIキー設定への誘導メッセージ）

**モデル設定確定の動作**:
1. 必須役割チェック: 全7役割にモデルが割り当てられているか確認
2. 未設定時: 警告ダイアログで不足役割を表示
3. 設定完了時: `config/llmmix_models.json` に保存
4. 次回起動時: 保存設定を自動復元

**設定ファイル形式** (`config/llmmix_models.json`):

```json
{
  "confirmed": true,
  "confirmed_at": "2026-02-02T12:00:00",
  "ollama_url": "http://localhost:11434",
  "role_assignments": {
    "supervisor": {"value": "claude-opus-4-5", "display": "[Claude] Opus 4.5 (最高性能)"},
    "router": {"value": "ollama:llama3.2:8b", "display": "[Ollama] llama3.2:8b"},
    ...
  }
}
```

### シミュレーション/実呼び出しの現状ステータス

| 機能 | ステータス | 説明 |
|------|------------|------|
| モデル一覧取得 | ✅ 実装済み | Ollama APIから取得 |
| 接続テスト | ✅ 実装済み | Ollamaへのping相当 |
| 役割別モデルテスト | ✅ 実装済み | Ollamaのみ |
| オーケストレーション実行 | ⚠️ シミュレーション | 実LLM呼び出しは未実装 |

**シミュレーションモード表示**:
- 設定パネル: 「⚠️ 現在: シミュレーションモード」（オレンジ背景）
- チャットパネル: 「⚠️ シミュレーションモード」（小さいラベル）

---

## ファイル変更一覧 (v3.9.1)

| ファイル | 変更内容 |
|----------|----------|
| `src/tabs/helix_orchestrator_tab.py` | モデル一覧表示バグ修正、プロバイダ表記統一、シミュレーション表示追加、設定復元機能 |
| `src/utils/constants.py` | バージョン 3.9.0 → 3.9.1 |
| `BIBLE_Helix AI Studio_3.9.1.md` | 本ファイル追加 |

---

## 技術詳細 (v3.9.1)

### 新規追加メソッド

| メソッド | 説明 |
|----------|------|
| `_populate_ollama_models_table(client, url)` | テーブルとコンボボックスを更新する共通メソッド |
| `_update_role_combos_with_ollama_models(names)` | 役割コンボボックスにOllamaモデルを追加 |
| `_restore_saved_model_settings()` | 起動時に保存済み設定を復元 |

### 変更されたメソッド

| メソッド | 変更内容 |
|----------|----------|
| `_test_ollama_connection()` | 成功時に `_populate_ollama_models_table()` を呼び出し |
| `_refresh_ollama_models()` | 共通メソッドを呼び出すようにリファクタリング |
| `_confirm_model_settings()` | 必須役割チェック、タイムスタンプ保存追加 |
| `_load_config()` | `llmmix_models.json` も読み込むように拡張 |

---

## 今後の展望

1. **LLMmix実呼び出し実装**: シミュレーションから実際のLLM呼び出しへ移行
2. **Claudeテスト機能**: API/CLI設定に応じたClaude疎通テスト
3. **役割別プロンプトカスタマイズ**: 各役割のシステムプロンプト編集機能
4. **実行履歴・ログ**: オーケストレーション結果の保存・参照

---

## 前バージョンからの継続事項

v3.9.0 で実装された以下の機能は継続:

- 4タブ構成（Claude / LLMmix / チャット作成 / 一般設定）
- 各タブのサブタブ構造
- Gemini Designer削除
- Knowledge / Encyclopedia機能
