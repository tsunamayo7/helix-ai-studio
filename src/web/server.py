"""
Helix AI Studio - Web UIã‚µãƒ¼ãƒãƒ¼ (v9.3.0)

FastAPI + Uvicornã‚µãƒ¼ãƒãƒ¼ã€‚
PyQt6ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§èµ·å‹•ã—ã€
å…±æœ‰ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆClaude CLI, RAGBuildLockç­‰ï¼‰ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã€‚

èµ·å‹•æ–¹æ³•:
  1. ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³: python -m src.web.server
  2. PyQt6çµ±åˆ: HelixAIStudio.py ã®è¨­å®šç”»é¢ã‹ã‚‰ãƒˆã‚°ãƒ«ã§èµ·å‹•

æŠ€è¡“çš„ãªæ³¨æ„:
  - FastAPI (asyncio) ã¨ PyQt6 (QEventLoop) ã¯åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œ
  - ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã¯ç¾æ™‚ç‚¹ã§ã¯ä¸è¦ï¼ˆClaude CLIã¯éƒ½åº¦subprocesså®Ÿè¡Œã®ãŸã‚ï¼‰
  - RAGBuildLockã®å…±æœ‰ã¯Phase 2ä»¥é™ã§å¯¾å¿œ
"""

import asyncio
import json
import logging
import os
import secrets
import subprocess
import time as _time
import uuid
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles

from .auth import WebAuthManager
from .api_routes import router as api_router
from .rag_bridge import WebRAGBridge
from .chat_store import ChatStore
from .ws_manager import WebSocketManager

# v11.5.3: Discordé€šçŸ¥
try:
    from ..utils.discord_notifier import notify_discord as _notify_discord
except ImportError:
    def _notify_discord(*args, **kwargs): return False

logger = logging.getLogger(__name__)

# =============================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹
# =============================================================================

rag_bridge = WebRAGBridge()
chat_store = ChatStore()

ws_manager = WebSocketManager(max_connections=3)
auth_manager = WebAuthManager()

_app_state = {
    "pyqt_running": False,
    "active_websockets": 0,
    "rag_locked": False,
}


def get_app_state() -> dict:
    """API routesã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®çŠ¶æ…‹å–å¾—"""
    _app_state["active_websockets"] = ws_manager.active_count
    return _app_state


# =============================================================================
# v9.5.0: Webå®Ÿè¡Œãƒ­ãƒƒã‚¯
# =============================================================================

LOCK_FILE = Path("data/web_execution_lock.json")


def _set_execution_lock(tab: str, client_info: str, prompt: str):
    """Webå®Ÿè¡Œãƒ­ãƒƒã‚¯ã‚’è¨­å®š"""
    lock_data = {
        "locked": True,
        "tab": tab,
        "client_info": client_info,
        "started_at": datetime.now().isoformat(),
        "prompt_preview": prompt[:50],
    }
    LOCK_FILE.parent.mkdir(parents=True, exist_ok=True)
    LOCK_FILE.write_text(json.dumps(lock_data, ensure_ascii=False), encoding='utf-8')


def _release_execution_lock():
    """Webå®Ÿè¡Œãƒ­ãƒƒã‚¯ã‚’è§£é™¤"""
    try:
        LOCK_FILE.write_text('{"locked": false}', encoding='utf-8')
    except Exception as e:
        logger.warning(f"[server] Failed to release execution lock: {e}")


# =============================================================================
# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# =============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """èµ·å‹•/çµ‚äº†ãƒ•ãƒƒã‚¯"""
    logger.info("Helix AI Studio Web Server starting...")
    logger.info(f"Port: {os.environ.get('HELIX_WEB_PORT', 8500)}")
    yield
    logger.info("Helix AI Studio Web Server shutting down...")


app = FastAPI(
    title="Helix AI Studio Web API",
    version="10.0.0",
    lifespan=lifespan,
)

# CORSè¨­å®š
# v9.9.2: allow_credentials=True ã¨ allow_origins=["*"] ã®çµ„ã¿åˆã‚ã›ã¯
# ãƒ–ãƒ©ã‚¦ã‚¶ãŒæ‹’å¦ã™ã‚‹ãŸã‚ã€æ˜ç¤ºçš„ãªã‚ªãƒªã‚¸ãƒ³ãƒªã‚¹ãƒˆã«å¤‰æ›´ã€‚
# Tailscale VPNçµŒç”±ã®ãƒ­ãƒ¼ã‚«ãƒ«/ãƒ¢ãƒã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚’æƒ³å®šã€‚
_CORS_ORIGINS = [
    "http://localhost:8500",
    "http://127.0.0.1:8500",
    "http://localhost:3000",
    "http://127.0.0.1:3000",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=_CORS_ORIGINS,
    # v10.0.0: Tailscale IP + ãƒã‚·ãƒ³åãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ã®ä¸¡æ–¹ã‚’è¨±å¯
    allow_origin_regex=r"http://(100\.\d+\.\d+\.\d+|[a-zA-Z0-9\-]+)(:\d+)?",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# REST APIãƒ«ãƒ¼ã‚¿ãƒ¼
app.include_router(api_router)

# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é™çš„ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡ï¼ˆStaticFilesã®ãƒ«ãƒ¼ãƒˆãƒã‚¦ãƒ³ãƒˆã‚’é¿ã‘ã‚‹ï¼‰
# ãƒ«ãƒ¼ãƒˆãƒã‚¦ãƒ³ãƒˆ("/")ã¯WebSocketãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ¨ªå–ã‚Šã—ã¦AssertionErrorã‚’èµ·ã“ã™ãŸã‚ã€
# /assets ã®ã¿StaticFilesã§ãƒã‚¦ãƒ³ãƒˆã—ã€ãã‚Œä»¥å¤–ã¯catch-all GETãƒ«ãƒ¼ãƒˆã§SPAå¯¾å¿œã™ã‚‹
frontend_dist = Path(__file__).parent.parent.parent / "frontend" / "dist"
if frontend_dist.exists():
    assets_dir = frontend_dist / "assets"
    if assets_dir.exists():
        app.mount("/assets", StaticFiles(directory=str(assets_dir)), name="assets")


# =============================================================================
# WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (cloudAI / æ—§soloAI)
# =============================================================================

async def _websocket_cloud_handler(websocket: WebSocket, token: str):
    """
    cloudAI WebSocketã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (v10.1.0: æ—§soloAI)ã€‚
    æ¥ç¶šæ™‚ã«JWTèªè¨¼ã‚’è¡Œã„ã€èªè¨¼æˆåŠŸå¾Œã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ä¿¡ã™ã‚‹ã€‚

    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ â†’ ã‚µãƒ¼ãƒãƒ¼ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:
      {"action": "execute", "prompt": "...", "model_id": "...", ...}
      {"action": "cancel"}
      {"action": "ping"}

    ã‚µãƒ¼ãƒãƒ¼ â†’ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:
      {"type": "streaming", "chunk": "...", "done": false}
      {"type": "streaming", "chunk": "...", "done": true}
      {"type": "status", "status": "...", "detail": "..."}
      {"type": "error", "error": "..."}
      {"type": "pong"}
    """
    # JWTèªè¨¼
    client_ip = websocket.client.host
    if not auth_manager.check_ip(client_ip):
        await websocket.close(code=4003, reason="IP not allowed")
        return

    payload = auth_manager.verify_token(token)
    if payload is None:
        await websocket.close(code=4001, reason="Invalid token")
        return

    # æ¥ç¶šå—ã‘å…¥ã‚Œ
    client_id = str(uuid.uuid4())
    connected = await ws_manager.connect(websocket, client_id)
    if not connected:
        await websocket.close(code=4029, reason="Too many connections")
        return

    try:
        await ws_manager.send_status(client_id, "connected", "cloudAI WebSocket ready")

        while True:
            data = await websocket.receive_json()
            action = data.get("action")

            if action == "ping":
                await ws_manager.send_to(client_id, {"type": "pong"})

            elif action == "execute":
                await _handle_solo_execute(client_id, data)

            elif action == "cancel":
                # Phase 1ã§ã¯æœªå®Ÿè£…ï¼ˆClaude CLIã¯subprocessãªã®ã§killãŒå¿…è¦ï¼‰
                await ws_manager.send_status(client_id, "cancelled", "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã¯ç¾åœ¨æœªå¯¾å¿œã§ã™")

            else:
                await ws_manager.send_error(client_id, f"Unknown action: {action}")

    except WebSocketDisconnect:
        logger.info(f"WebSocket client disconnected: {client_id}")
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        await ws_manager.disconnect(client_id)


@app.websocket("/ws/cloud")
async def websocket_cloud(websocket: WebSocket, token: str = Query(...)):
    await _websocket_cloud_handler(websocket, token)


@app.websocket("/ws/solo")
async def websocket_solo_compat(websocket: WebSocket, token: str = Query(...)):
    """v10.1.0: å¾Œæ–¹äº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆ/ws/solo â†’ /ws/cloud ã¨åŒã˜ãƒãƒ³ãƒ‰ãƒ©ï¼‰"""
    await _websocket_cloud_handler(websocket, token)


# =============================================================================
# WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (mixAI)
# =============================================================================

@app.websocket("/ws/mix")
async def websocket_mix(websocket: WebSocket, token: str = Query(...)):
    """
    mixAI WebSocketã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚
    3Phaseå®Ÿè¡Œã®é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é…ä¿¡ã€‚

    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ â†’ ã‚µãƒ¼ãƒãƒ¼:
      {"action": "execute", "prompt": "...", "model_id": "...",
       "model_assignments": {...}, "project_dir": "...", "attached_files": [...]}
      {"action": "cancel"}

    ã‚µãƒ¼ãƒãƒ¼ â†’ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ:
      {"type": "phase_changed", "phase": 1, "description": "..."}
      {"type": "streaming", "chunk": "...", "done": false}
      {"type": "llm_started", "category": "coding", "model": "devstral-2:123b"}
      {"type": "llm_finished", "category": "coding", "success": true, "elapsed": 12.5}
      {"type": "phase2_progress", "completed": 2, "total": 5}
      {"type": "streaming", "chunk": "...", "done": true}
      {"type": "error", "error": "..."}
    """
    # JWTèªè¨¼ï¼ˆcloudAIã¨åŒã˜ï¼‰
    client_ip = websocket.client.host
    if not auth_manager.check_ip(client_ip):
        await websocket.close(code=4003, reason="IP not allowed")
        return

    payload = auth_manager.verify_token(token)
    if payload is None:
        await websocket.close(code=4001, reason="Invalid token")
        return

    client_id = str(uuid.uuid4())
    connected = await ws_manager.connect(websocket, client_id)
    if not connected:
        await websocket.close(code=4029, reason="Too many connections")
        return

    try:
        await ws_manager.send_status(client_id, "connected", "mixAI WebSocket ready")

        while True:
            data = await websocket.receive_json()
            action = data.get("action")

            if action == "ping":
                await ws_manager.send_to(client_id, {"type": "pong"})
            elif action == "execute":
                await _handle_mix_execute(client_id, data)
            elif action == "cancel":
                await ws_manager.send_status(client_id, "cancelled", "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã¯ç¾åœ¨æœªå¯¾å¿œã§ã™")
            else:
                await ws_manager.send_error(client_id, f"Unknown action: {action}")

    except WebSocketDisconnect:
        logger.info(f"mixAI WebSocket disconnected: {client_id}")
    except Exception as e:
        logger.error(f"mixAI WebSocket error: {e}")
    finally:
        await ws_manager.disconnect(client_id)


# =============================================================================
# WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (localAI)  v11.5.3
# =============================================================================

@app.websocket("/ws/local")
async def websocket_local(websocket: WebSocket, token: str = Query(...)):
    """
    localAI WebSocketã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã€‚
    Ollama API ã‚’ç›´æ¥å‘¼ã³å‡ºã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’è¿”ã™ã€‚

    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ â†’ ã‚µãƒ¼ãƒãƒ¼:
      {"action": "execute", "prompt": "...", "model": "gemma3:27b",
       "chat_id": "...", "attached_files": [...]}
      {"action": "cancel"}

    ã‚µãƒ¼ãƒãƒ¼ â†’ ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ:
      {"type": "streaming", "chunk": "...", "done": false}
      {"type": "streaming", "chunk": "...", "done": true}
      {"type": "status", "status": "...", "detail": "..."}
      {"type": "error", "error": "..."}
    """
    # JWTèªè¨¼
    client_ip = websocket.client.host
    if not auth_manager.check_ip(client_ip):
        await websocket.close(code=4003, reason="IP not allowed")
        return

    payload = auth_manager.verify_token(token)
    if payload is None:
        await websocket.close(code=4001, reason="Invalid token")
        return

    client_id = str(uuid.uuid4())
    connected = await ws_manager.connect(websocket, client_id)
    if not connected:
        await websocket.close(code=4029, reason="Too many connections")
        return

    try:
        await ws_manager.send_status(client_id, "connected", "localAI WebSocket ready")

        while True:
            data = await websocket.receive_json()
            action = data.get("action")

            if action == "ping":
                await ws_manager.send_to(client_id, {"type": "pong"})

            elif action == "execute":
                await _handle_local_execute(client_id, data)

            elif action == "cancel":
                await ws_manager.send_status(client_id, "cancelled", "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã¯ç¾åœ¨æœªå¯¾å¿œã§ã™")

            else:
                await ws_manager.send_error(client_id, f"Unknown action: {action}")

    except WebSocketDisconnect:
        logger.info(f"localAI WebSocket disconnected: {client_id}")
    except Exception as e:
        logger.error(f"localAI WebSocket error: {e}")
    finally:
        await ws_manager.disconnect(client_id)


async def _handle_local_execute(client_id: str, data: dict):
    """
    localAIå®Ÿè¡Œãƒãƒ³ãƒ‰ãƒ© (v11.5.3)
    Ollama API ã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒãƒ£ãƒƒãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    """
    prompt = data.get("prompt", "")
    chat_id = data.get("chat_id")
    model = data.get("model", "")
    attached_files = data.get("attached_files", [])
    client_info = data.get("client_info", "Web Client")

    if not prompt:
        await ws_manager.send_error(client_id, "Prompt is empty")
        return

    # è¨­å®šèª­ã¿è¾¼ã¿ï¼ˆollama_hostç­‰ã«å¿…è¦ï¼‰
    settings = _load_merged_settings()

    # ãƒ¢ãƒ‡ãƒ«æœªæŒ‡å®šãªã‚‰è¨­å®šã‹ã‚‰å–å¾—
    if not model:
        try:
            assignments = settings.get("model_assignments", {})
            model = assignments.get("coding") or ""
        except Exception:
            pass
    if not model:
        await ws_manager.send_error(client_id, "No model specified. Please select a model.")
        return

    ollama_host = settings.get("ollama_host", "http://localhost:11434")

    # å®Ÿè¡Œãƒ­ãƒƒã‚¯
    _set_execution_lock("localAI", client_info, prompt)

    # ãƒãƒ£ãƒƒãƒˆIDç®¡ç†
    if not chat_id:
        chat = chat_store.create_chat(tab="localAI")
        chat_id = chat["id"]
        await ws_manager.send_to(client_id, {
            "type": "chat_created",
            "chat_id": chat_id,
        })

    chat_store.add_message(chat_id, "user", prompt)

    # ã‚¿ã‚¤ãƒˆãƒ«è‡ªå‹•ç”Ÿæˆ
    chat = chat_store.get_chat(chat_id)
    if chat and chat["message_count"] == 1:
        title = chat_store.auto_generate_title(chat_id)
        await ws_manager.send_to(client_id, {
            "type": "chat_title_updated",
            "chat_id": chat_id,
            "title": title,
        })

    # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
    full_prompt = prompt
    if attached_files:
        full_prompt += "\n\n[æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«]\n" + "\n".join(f"- {f}" for f in attached_files)

    ws_manager.set_active_task(client_id, "localAI")
    await ws_manager.send_status(client_id, "executing", f"{model} å®Ÿè¡Œä¸­...")

    # Discord: ãƒãƒ£ãƒƒãƒˆé–‹å§‹é€šçŸ¥
    _notify_discord("localAI", "started", f"ãƒ¢ãƒ‡ãƒ«: {model}\n{prompt[:200]}")

    start_time = _time.time()

    try:
        response = await _run_ollama_async(
            model=model,
            prompt=full_prompt,
            timeout=_get_claude_timeout_sec(),
            host=ollama_host,
        )

        elapsed = _time.time() - start_time

        # ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒªãƒ¼ã‚’æœ«å°¾ã«ä»˜è¨˜
        model_summary = f"\n\n---\nğŸ¤– **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ« (Ollama)**: `{model}`"
        response_with_summary = response + model_summary

        # å¿œç­”ä¿å­˜ï¼ˆã‚µãƒãƒªãƒ¼è¾¼ã¿ï¼‰
        chat_store.add_message(chat_id, "assistant", response_with_summary,
                               metadata={"model": model, "elapsed": round(elapsed, 1)})

        await ws_manager.send_streaming(client_id, response_with_summary, done=True)
        await ws_manager.send_status(client_id, "completed", f"å®Ÿè¡Œå®Œäº† ({elapsed:.1f}s)")

        # Discord: å®Œäº†é€šçŸ¥
        _notify_discord("localAI", "completed", response[:500], elapsed=elapsed)

        # ä¼šè©±ã‚’RAGã«ä¿å­˜
        asyncio.ensure_future(_save_web_conversation(
            [{"role": "user", "content": prompt},
             {"role": "assistant", "content": response}],
            tab="localAI",
        ))

    except Exception as e:
        elapsed = _time.time() - start_time
        error_msg = str(e)

        # Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ã‚ã‹ã‚Šã‚„ã™ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¤‰æ›
        if "ConnectError" in error_msg or "Connection refused" in error_msg:
            error_msg = f"Ollama ã«æ¥ç¶šã§ãã¾ã›ã‚“ ({ollama_host})ã€‚Ollama ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        elif "404" in error_msg:
            error_msg = f"ãƒ¢ãƒ‡ãƒ« '{model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Ollama ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ«ã—ã¦ãã ã•ã„ã€‚"

        await ws_manager.send_error(client_id, error_msg)

        # Discord: ã‚¨ãƒ©ãƒ¼é€šçŸ¥
        _notify_discord("localAI", "error", prompt[:200], error=error_msg)

    finally:
        _release_execution_lock()
        ws_manager.set_active_task(client_id, None)


# =============================================================================
# SPA catch-all ãƒ«ãƒ¼ãƒˆï¼ˆWebSocketãƒ«ãƒ¼ãƒˆã‚ˆã‚Šå¾Œã«å®šç¾©ï¼‰
# =============================================================================

if frontend_dist.exists():
    # SPAå¯¾å¿œ: 404ã‚¨ãƒ©ãƒ¼æ™‚ã«index.htmlã‚’è¿”ã™ï¼ˆcatch-allãƒ«ãƒ¼ãƒˆã®ä»£ã‚ã‚Šï¼‰
    # ã“ã‚Œã«ã‚ˆã‚ŠAPIãƒ«ãƒ¼ãƒˆã¨ã®ç«¶åˆã‚’å®Œå…¨ã«å›é¿ã§ãã‚‹
    from starlette.exceptions import HTTPException as StarletteHTTPException
    from starlette.requests import Request as StarletteRequest

    _original_index = frontend_dist / "index.html"

    @app.exception_handler(404)
    async def spa_not_found_handler(request: StarletteRequest, exc: StarletteHTTPException):
        """404æ™‚ã«SPAã®index.htmlã‚’è¿”ã™ï¼ˆAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯é™¤å¤–ï¼‰"""
        path = request.url.path
        # APIã‚„docsãƒ‘ã‚¹ã¯é€šå¸¸ã®404ã‚’è¿”ã™
        if path.startswith(("/api/", "/ws/", "/docs", "/openapi.json", "/redoc")):
            return JSONResponse(
                status_code=exc.status_code,
                content={"detail": exc.detail or "Not found"},
            )
        # é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚Œã°ãã‚Œã‚’è¿”ã™
        file_path = frontend_dist / path.lstrip("/")
        if file_path.is_file() and ".." not in path:
            return FileResponse(file_path)
        # ãã‚Œä»¥å¤–ã¯SPAã®index.htmlã‚’è¿”ã™
        return FileResponse(_original_index)


async def _handle_solo_execute(client_id: str, data: dict):
    """
    cloudAIå®Ÿè¡Œãƒãƒ³ãƒ‰ãƒ© (v10.1.0: æ—§soloAI, v9.2.0: ChatStoreçµ±åˆ)ã€‚
    Claude CLIã‚’subprocessã§å®Ÿè¡Œã—ã€çµæœã‚’WebSocketã§é€ä¿¡ã€‚
    """
    from ..utils.subprocess_utils import run_hidden

    prompt = data.get("prompt", "")
    chat_id = data.get("chat_id")  # v9.2.0
    model_id = data.get("model_id", "") or _get_default_model()
    project_dir = data.get("project_dir", "")
    timeout = data.get("timeout") or 0
    timeout = timeout if timeout > 0 else _get_claude_timeout_sec()
    use_mcp = data.get("use_mcp", True)
    auto_approve = data.get("auto_approve", True)
    enable_rag = data.get("enable_rag", True)
    attached_files = data.get("attached_files", [])
    client_info = data.get("client_info", "Web Client")  # v9.5.0

    if not prompt:
        await ws_manager.send_error(client_id, "Prompt is empty")
        return

    # v9.5.0: Webå®Ÿè¡Œãƒ­ãƒƒã‚¯è¨­å®š
    _set_execution_lock("cloudAI", client_info, prompt)

    # v9.2.0: ãƒãƒ£ãƒƒãƒˆIDãŒãªã„å ´åˆã¯æ–°è¦ä½œæˆ
    if not chat_id:
        chat = chat_store.create_chat(tab="cloudAI")
        chat_id = chat["id"]
        await ws_manager.send_to(client_id, {
            "type": "chat_created",
            "chat_id": chat_id,
        })

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¿å­˜
    chat_store.add_message(chat_id, "user", prompt)

    # ã‚¿ã‚¤ãƒˆãƒ«è‡ªå‹•ç”Ÿæˆï¼ˆæœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ™‚ï¼‰
    chat = chat_store.get_chat(chat_id)
    if chat and chat["message_count"] == 1:
        title = chat_store.auto_generate_title(chat_id)
        await ws_manager.send_to(client_id, {
            "type": "chat_title_updated",
            "chat_id": chat_id,
            "title": title,
        })

    # v9.2.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    context_result = chat_store.build_context_for_prompt(chat_id, prompt)
    full_prompt = context_result["prompt"]

    # ãƒˆãƒ¼ã‚¯ãƒ³è­¦å‘Š
    if context_result.get("warning"):
        await ws_manager.send_to(client_id, {
            "type": "token_warning",
            "message": context_result["warning"],
            "token_estimate": context_result["token_estimate"],
        })

    # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥ï¼ˆãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰ä»¥å¤–ï¼‰
    if enable_rag and context_result["mode"] != "full":
        try:
            rag_context = await rag_bridge.build_context(prompt, tab="cloudAI")
            if rag_context:
                full_prompt = f"{rag_context}\n\n{full_prompt}"
                await ws_manager.send_to(client_id, {
                    "type": "status",
                    "status": "rag_injected",
                    "message": f"RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥: {len(rag_context)}æ–‡å­—",
                })
        except Exception as e:
            logger.warning(f"RAG context build failed: {e}")

    # ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜æƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
    if attached_files:
        file_lines = [f"- {f}" for f in attached_files]
        full_prompt += f"\n\n[æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«]\n" + "\n".join(file_lines)

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: å®Ÿè¡Œä¸­
    ws_manager.set_active_task(client_id, "cloudAI")
    await ws_manager.send_status(client_id, "executing", f"Claude ({model_id}) å®Ÿè¡Œä¸­...")

    # v11.5.3: Discordé–‹å§‹é€šçŸ¥
    _notify_discord("cloudAI", "started", f"ãƒ¢ãƒ‡ãƒ«: {model_id}\n{prompt[:200]}")

    # Claude CLIæ§‹ç¯‰
    cmd = [
        "claude",
        "-p",
        "--output-format", "json",
        "--model", model_id,
    ]
    if auto_approve:
        cmd.append("--dangerously-skip-permissions")

    run_cwd = project_dir if project_dir and os.path.isdir(project_dir) else None

    try:
        result = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: run_hidden(
                cmd,
                input=full_prompt,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace',
                timeout=timeout,
                env={**os.environ, "FORCE_COLOR": "0", "PYTHONIOENCODING": "utf-8"},
                cwd=run_cwd,
            )
        )

        stdout = result.stdout or ""
        stderr = result.stderr or ""

        if result.returncode == 0:
            try:
                output_data = json.loads(stdout)
                response_text = output_data.get("result", stdout)
            except json.JSONDecodeError:
                response_text = stdout.strip()

            # ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒªãƒ¼ã‚’æœ«å°¾ã«ä»˜è¨˜
            model_summary = f"\n\n---\nğŸ¤– **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«**: `{model_id}`"
            response_with_summary = response_text + model_summary

            # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ä¿å­˜ï¼ˆã‚µãƒãƒªãƒ¼è¾¼ã¿ï¼‰
            chat_store.add_message(chat_id, "assistant", response_with_summary,
                                   metadata={"model": model_id, "mode": context_result["mode"],
                                             "tokens_estimated": context_result["token_estimate"]})

            # å®Œäº†é€ä¿¡ï¼ˆã‚µãƒãƒªãƒ¼è¾¼ã¿ï¼‰
            await ws_manager.send_streaming(client_id, response_with_summary, done=True)
            await ws_manager.send_status(client_id, "completed", "å®Ÿè¡Œå®Œäº†")

            # v11.5.3: Discordå®Œäº†é€šçŸ¥
            _notify_discord("cloudAI", "completed", response_text[:500])

            # ä¼šè©±ã‚’RAGã«ä¿å­˜
            asyncio.ensure_future(_save_web_conversation(
                [{"role": "user", "content": prompt},
                 {"role": "assistant", "content": response_text}],
                tab="cloudAI",
            ))
        else:
            error_msg = f"Claude CLI error (code {result.returncode}): {stderr[:500]}"
            chat_store.add_message(chat_id, "error", error_msg)
            await ws_manager.send_error(client_id, error_msg)

    except subprocess.TimeoutExpired:
        await ws_manager.send_error(client_id, f"Claude CLI timed out ({timeout}s)")
        _notify_discord("cloudAI", "timeout", prompt[:200], error=f"Timeout {timeout}s")
    except FileNotFoundError:
        await ws_manager.send_error(client_id, "Claude CLI not found")
        _notify_discord("cloudAI", "error", prompt[:200], error="Claude CLI not found")
    except Exception as e:
        await ws_manager.send_error(client_id, f"Execution error: {str(e)}")
        _notify_discord("cloudAI", "error", prompt[:200], error=str(e))
    finally:
        _release_execution_lock()  # v9.5.0
        ws_manager.set_active_task(client_id, None)


# =============================================================================
# mixAI 3Phaseå®Ÿè¡Œãƒãƒ³ãƒ‰ãƒ©
# =============================================================================

async def _handle_mix_execute(client_id: str, data: dict):
    """
    mixAI 3Phaseå®Ÿè¡Œãƒãƒ³ãƒ‰ãƒ©ã€‚

    MixAIOrchestratorã¯QThread(PyQt6)å‰æã®ãŸã‚ã€Webç‰ˆã§ã¯
    ç›´æ¥Claude CLIã¨Ollama APIã‚’å‘¼ã³å‡ºã™è»½é‡ç‰ˆã‚’å®Ÿè£…ã™ã‚‹ã€‚

    Phase 1: Claude CLI â†’ è¨ˆç”»JSON + claude_answer
    Phase 2: Ollama API â†’ ãƒ­ãƒ¼ã‚«ãƒ«LLMé †æ¬¡å®Ÿè¡Œ
    Phase 3: Claude CLI â†’ æ¯”è¼ƒçµ±åˆ â†’ æœ€çµ‚å›ç­”
    """
    prompt = data.get("prompt", "")
    chat_id = data.get("chat_id")  # v9.2.0
    model_id = data.get("model_id", "") or _get_default_model()
    # v9.3.0: orchestrator_engineï¼ˆconfig.jsonã‹ã‚‰èª­ã¿å–ã‚Šï¼‰
    engine_id = _load_orchestrator_engine()
    model_assignments = data.get("model_assignments", {})
    project_dir = data.get("project_dir", "")
    attached_files = data.get("attached_files", [])
    timeout = data.get("timeout") or 0
    timeout = timeout if timeout > 0 else _get_claude_timeout_sec()
    enable_rag = data.get("enable_rag", True)
    client_info = data.get("client_info", "Web Client")  # v9.5.0

    if not prompt:
        await ws_manager.send_error(client_id, "Prompt is empty")
        return

    # v9.5.0: Webå®Ÿè¡Œãƒ­ãƒƒã‚¯è¨­å®š
    _set_execution_lock("mixAI", client_info, prompt)

    # v9.2.0: ãƒãƒ£ãƒƒãƒˆIDãŒãªã„å ´åˆã¯æ–°è¦ä½œæˆ
    if not chat_id:
        chat = chat_store.create_chat(tab="mixAI")
        chat_id = chat["id"]
        await ws_manager.send_to(client_id, {
            "type": "chat_created",
            "chat_id": chat_id,
        })

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¿å­˜
    chat_store.add_message(chat_id, "user", prompt)

    # ã‚¿ã‚¤ãƒˆãƒ«è‡ªå‹•ç”Ÿæˆ
    chat = chat_store.get_chat(chat_id)
    if chat and chat["message_count"] == 1:
        title = chat_store.auto_generate_title(chat_id)
        await ws_manager.send_to(client_id, {
            "type": "chat_title_updated",
            "chat_id": chat_id,
            "title": title,
        })

    # v9.2.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    context_result = chat_store.build_context_for_prompt(chat_id, prompt)
    rag_prompt = context_result["prompt"]

    ws_manager.set_active_task(client_id, "mixAI")

    # v11.5.3: Discordé–‹å§‹é€šçŸ¥
    _notify_discord("mixAI", "started", f"ã‚¨ãƒ³ã‚¸ãƒ³: {engine_id}\n{prompt[:200]}")

    # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥ï¼ˆãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰ä»¥å¤–ï¼‰
    if enable_rag and context_result["mode"] != "full":
        try:
            rag_context = await rag_bridge.build_context(prompt, tab="mixAI")
            if rag_context:
                rag_prompt = f"{rag_context}\n\n{rag_prompt}"
                await ws_manager.send_to(client_id, {
                    "type": "status",
                    "status": "rag_injected",
                    "message": f"RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ³¨å…¥: {len(rag_context)}æ–‡å­—",
                })
        except Exception as e:
            logger.warning(f"RAG context build failed (mixAI): {e}")

    try:
        # â•â•â• Phase 1: è¨ˆç”»ç«‹æ¡ˆï¼ˆv9.3.0: ã‚¨ãƒ³ã‚¸ãƒ³åˆ†å²ï¼‰ â•â•â•
        engine_label = "ãƒ­ãƒ¼ã‚«ãƒ«LLM" if not _is_claude_engine(engine_id) else "Claude"
        await ws_manager.send_to(client_id, {
            "type": "phase_changed",
            "phase": 1,
            "description": f"Phase 1: {engine_label}è¨ˆç”»ç«‹æ¡ˆä¸­...",
        })

        if _is_claude_engine(engine_id):
            phase1_result = await _run_claude_cli_async(
                prompt=_build_phase1_prompt(rag_prompt),
                model_id=engine_id,
                project_dir=project_dir,
                timeout=timeout,
            )
        else:
            phase1_result = await _run_local_agent(
                prompt=_build_phase1_prompt(rag_prompt),
                model_name=engine_id,
                project_dir=project_dir,
                phase="p1",
            )

        # Phase 1çµæœãƒ‘ãƒ¼ã‚¹
        claude_answer = phase1_result.get("claude_answer", "")
        llm_instructions = phase1_result.get("local_llm_instructions", {})
        complexity = phase1_result.get("complexity", "low")
        skip_phase2 = phase1_result.get("skip_phase2", False)

        # Phase 1ã®Claudeå›ç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€ä¿¡
        if claude_answer:
            await ws_manager.send_to(client_id, {
                "type": "streaming",
                "chunk": f"**[Phase 1 Claudeå›ç­”]**\n{claude_answer[:500]}...\n\n",
                "done": False,
            })

        # complexityãŒlowã¾ãŸã¯skip_phase2ã®å ´åˆã€Phase 2-3ã‚¹ã‚­ãƒƒãƒ—
        if skip_phase2 or complexity == "low":
            await ws_manager.send_streaming(client_id, claude_answer, done=True)
            await ws_manager.send_status(client_id, "completed", "Phase 2-3ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä½è¤‡é›‘åº¦ï¼‰")
            return

        # â•â•â• Phase 2: ãƒ­ãƒ¼ã‚«ãƒ«LLMé †æ¬¡å®Ÿè¡Œ â•â•â•
        await ws_manager.send_to(client_id, {
            "type": "phase_changed",
            "phase": 2,
            "description": "Phase 2: ãƒ­ãƒ¼ã‚«ãƒ«LLMé †æ¬¡å®Ÿè¡Œä¸­...",
        })

        tasks = _build_phase2_tasks(llm_instructions, model_assignments)
        phase2_results = []
        total_tasks = len(tasks)

        for i, task in enumerate(tasks):
            # LLMé–‹å§‹é€šçŸ¥
            await ws_manager.send_to(client_id, {
                "type": "llm_started",
                "category": task["category"],
                "model": task["model"],
            })

            # Ollama APIå‘¼ã³å‡ºã—
            start = _time.time()
            try:
                result = await _run_ollama_async(
                    model=task["model"],
                    prompt=task["prompt"],
                    timeout=task.get("timeout", 300),
                )
                elapsed = _time.time() - start
                phase2_results.append({
                    "category": task["category"],
                    "model": task["model"],
                    "response": result,
                    "success": True,
                    "elapsed": elapsed,
                })

                await ws_manager.send_to(client_id, {
                    "type": "llm_finished",
                    "category": task["category"],
                    "success": True,
                    "elapsed": round(elapsed, 1),
                })
            except Exception as e:
                elapsed = _time.time() - start
                phase2_results.append({
                    "category": task["category"],
                    "model": task["model"],
                    "response": str(e),
                    "success": False,
                    "elapsed": elapsed,
                })
                await ws_manager.send_to(client_id, {
                    "type": "llm_finished",
                    "category": task["category"],
                    "success": False,
                    "elapsed": round(elapsed, 1),
                })

            # é€²æ—é€šçŸ¥
            await ws_manager.send_to(client_id, {
                "type": "phase2_progress",
                "completed": i + 1,
                "total": total_tasks,
            })

        # â•â•â• Phase 3: æ¯”è¼ƒçµ±åˆï¼ˆv9.3.0: ã‚¨ãƒ³ã‚¸ãƒ³åˆ†å²ï¼‰ â•â•â•
        await ws_manager.send_to(client_id, {
            "type": "phase_changed",
            "phase": 3,
            "description": f"Phase 3: {engine_label}æ¯”è¼ƒçµ±åˆä¸­...",
        })

        phase3_prompt = _build_phase3_prompt(prompt, claude_answer, phase2_results)
        if _is_claude_engine(engine_id):
            phase3_result = await _run_claude_cli_async(
                prompt=phase3_prompt,
                model_id=engine_id,
                project_dir=project_dir,
                timeout=timeout,
            )
        else:
            phase3_result = await _run_local_agent(
                prompt=phase3_prompt,
                model_name=engine_id,
                project_dir=project_dir,
                phase="p3",
            )

        # æœ€çµ‚å›ç­”æŠ½å‡º
        if isinstance(phase3_result, dict):
            final_answer = phase3_result.get("final_answer", str(phase3_result))
        else:
            final_answer = str(phase3_result)

        # ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆï¼ˆPhase2 ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼‰
        p2_lines = []
        for r in phase2_results:
            status_icon = "âœ…" if r["success"] else "âŒ"
            p2_lines.append(f"  {status_icon} {r['category']}: `{r['model']}`")

        if p2_lines:
            p2_block = "\n- Phase 2 (ãƒ­ãƒ¼ã‚«ãƒ«LLM):\n" + "\n".join(p2_lines)
        else:
            p2_block = "\n- Phase 2: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä½è¤‡é›‘åº¦ï¼‰"

        model_summary = (
            f"\n\n---\nğŸ¤– **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«**\n"
            f"- Phase 1/3 (ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼): `{engine_id}`"
            f"{p2_block}"
        )
        final_answer_with_summary = final_answer + model_summary

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ä¿å­˜ï¼ˆã‚µãƒãƒªãƒ¼è¾¼ã¿ï¼‰
        chat_store.add_message(chat_id, "assistant", final_answer_with_summary,
                               metadata={"model": model_id, "mode": context_result["mode"]})

        await ws_manager.send_streaming(client_id, final_answer_with_summary, done=True)
        await ws_manager.send_status(client_id, "completed", "3Phaseå®Ÿè¡Œå®Œäº†")

        # v11.5.3: Discordå®Œäº†é€šçŸ¥
        _notify_discord("mixAI", "completed", final_answer[:500])

        # ä¼šè©±ã‚’RAGã«ä¿å­˜
        asyncio.ensure_future(_save_web_conversation(
            [{"role": "user", "content": prompt},
             {"role": "assistant", "content": final_answer}],
            tab="mixAI",
        ))

    except Exception as e:
        await ws_manager.send_error(client_id, f"mixAI execution error: {str(e)}")
        _notify_discord("mixAI", "error", prompt[:200], error=str(e))
    finally:
        _release_execution_lock()  # v9.5.0
        ws_manager.set_active_task(client_id, None)


# =============================================================================
# mixAI ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =============================================================================

async def _run_claude_cli_async(prompt: str, model_id: str,
                                project_dir: str = "", timeout: int = 0) -> dict:
    """Claude CLIã‚’éåŒæœŸã§å®Ÿè¡Œ"""
    if timeout <= 0:
        timeout = _get_claude_timeout_sec()
    from ..utils.subprocess_utils import run_hidden

    cmd = ["claude", "-p", "--output-format", "json", "--model", model_id,
           "--dangerously-skip-permissions"]

    run_cwd = project_dir if project_dir and os.path.isdir(project_dir) else None

    result = await asyncio.get_event_loop().run_in_executor(
        None,
        lambda: run_hidden(
            cmd, input=prompt, capture_output=True, text=True,
            encoding='utf-8', errors='replace', timeout=timeout,
            env={**os.environ, "FORCE_COLOR": "0", "PYTHONIOENCODING": "utf-8"},
            cwd=run_cwd,
        )
    )

    stdout = result.stdout or ""
    if result.returncode == 0:
        try:
            data = json.loads(stdout)
            text = data.get("result", stdout)
        except json.JSONDecodeError:
            text = stdout.strip()

        # JSONæ§‹é€ ã®æŠ½å‡ºã‚’è©¦è¡Œ
        try:
            start = text.find('{')
            end = text.rfind('}') + 1
            if start >= 0 and end > start:
                return json.loads(text[start:end])
        except (json.JSONDecodeError, ValueError):
            pass

        return {"claude_answer": text, "complexity": "low", "skip_phase2": True}
    else:
        raise RuntimeError(f"Claude CLI error (code {result.returncode})")


async def _run_ollama_async(model: str, prompt: str,
                            timeout: int = 300, host: str = "http://localhost:11434") -> str:
    """Ollama APIã‚’éåŒæœŸã§å‘¼ã³å‡ºã—"""
    import httpx

    async with httpx.AsyncClient(timeout=httpx.Timeout(timeout)) as client:
        resp = await client.post(
            f"{host}/api/generate",
            json={"model": model, "prompt": prompt, "stream": False},
        )
        resp.raise_for_status()
        return resp.json().get("response", "")


# =============================================================================
# v9.3.0: ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
# =============================================================================

def _get_default_model() -> str:
    """v11.5.0: cloud_models.json ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—"""
    try:
        from ..utils.constants import get_default_claude_model
        return get_default_claude_model()
    except Exception:
        return ""


def _get_claude_timeout_sec() -> int:
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Claudeã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰ã‚’èª­ã¿å–ã‚‹ã€‚

    å„ªå…ˆé †ä½:
      1. general_settings.json â†’ timeout_minutesï¼ˆãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—è¨­å®šç”»é¢ã®å€¤ï¼‰
      2. config.json â†’ timeoutï¼ˆç§’å˜ä½ï¼‰
      3. app_settings.json â†’ claude.timeout_minutes
      4. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5400ç§’ï¼ˆ90åˆ†ï¼‰
    """
    timeout_sec = 5400  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 90åˆ†

    # app_settings.jsonï¼ˆæœ€ä½å„ªå…ˆåº¦ï¼‰
    try:
        p = Path("config/app_settings.json")
        if p.exists():
            with open(p, 'r', encoding='utf-8') as f:
                d = json.load(f)
            v = d.get("claude", {}).get("timeout_minutes")
            if v and isinstance(v, (int, float)) and v > 0:
                timeout_sec = int(v) * 60
    except Exception:
        pass

    # config.jsonï¼ˆç§’å˜ä½ï¼‰
    try:
        p = Path("config/config.json")
        if p.exists():
            with open(p, 'r', encoding='utf-8') as f:
                d = json.load(f)
            v = d.get("timeout")
            if v and isinstance(v, (int, float)) and v > 0:
                timeout_sec = int(v)
    except Exception:
        pass

    # general_settings.jsonï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
    try:
        p = Path("config/general_settings.json")
        if p.exists():
            with open(p, 'r', encoding='utf-8') as f:
                d = json.load(f)
            v = d.get("timeout_minutes")
            if v and isinstance(v, (int, float)) and v > 0:
                timeout_sec = int(v) * 60
    except Exception:
        pass

    return timeout_sec


def _is_claude_engine(engine_id: str) -> bool:
    """Claude CLIã§å®Ÿè¡Œã™ã¹ãã‚¨ãƒ³ã‚¸ãƒ³ã‹ã©ã†ã‹"""
    return engine_id.startswith("claude-")


def _load_orchestrator_engine() -> str:
    """config.jsonã‹ã‚‰orchestrator_engineã‚’èª­ã¿å–ã‚Š"""
    try:
        config_path = Path("config/config.json")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config.get("orchestrator_engine") or ""
    except Exception as e:
        logger.warning(f"[server] Failed to load orchestrator engine: {e}")
    return ""


async def _run_local_agent(prompt: str, model_name: str,
                            project_dir: str, phase: str = "p1") -> dict:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’éåŒæœŸã§å®Ÿè¡Œ"""
    # local_agent.py è‡ªä½“ã¯PyQt6ã«ä¾å­˜ã—ãªã„ãŒã€
    # from ..backends.local_agent ã ã¨ backends/__init__.py ãŒå®Ÿè¡Œã•ã‚Œ
    # mix_orchestrator â†’ PyQt6 ã®é€£é–importãŒç™ºç”Ÿã™ã‚‹ã€‚
    # importlib.util ã§ .py ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ãƒ­ãƒ¼ãƒ‰ã—ã¦ __init__.py ã‚’ãƒã‚¤ãƒ‘ã‚¹ã™ã‚‹ã€‚
    import importlib.util as _ilu
    _spec = _ilu.spec_from_file_location(
        "src.backends.local_agent",
        str(Path(__file__).parent.parent / "backends" / "local_agent.py"),
    )
    _mod = _ilu.module_from_spec(_spec)
    _spec.loader.exec_module(_mod)
    LocalAgentRunner = _mod.LocalAgentRunner

    config_path = Path("config/config.json")
    tools_config = {}
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        tools_config = config.get("local_agent_tools", {})

    agent = LocalAgentRunner(
        model_name=model_name,
        project_dir=project_dir,
        tools_config=tools_config,
    )

    # Webç‰ˆã§ã¯æ›¸ãè¾¼ã¿è‡ªå‹•æ‰¿èª
    agent.on_write_confirm = lambda tool, path, preview: True

    system_prompt = _build_local_system_prompt(phase)
    result = await asyncio.to_thread(agent.run, system_prompt, prompt)

    # JSONæ§‹é€ ã®æŠ½å‡ºã‚’è©¦è¡Œ
    try:
        start = result.find('{')
        end = result.rfind('}') + 1
        if start >= 0 and end > start:
            return json.loads(result[start:end])
    except (json.JSONDecodeError, ValueError):
        pass

    if phase == "p1":
        return {"claude_answer": result, "complexity": "low", "skip_phase2": True}
    else:
        return {"status": "complete", "final_answer": result}


def _build_local_system_prompt(phase: str) -> str:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
    if phase == "p1":
        return """ã‚ãªãŸã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®è¨ˆç”»ç«‹æ¡ˆã‚’è¡Œã†AIã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã¾ãšãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹é€ ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã€
é©åˆ‡ãªè¨ˆç”»ã‚’ç«‹æ¡ˆã—ã¦ãã ã•ã„ã€‚

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:
- read_file: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚€
- list_dir: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§
- search_files: ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢

ã¾ãšãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’ç¢ºèªã—ã€é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã‚“ã§ã‹ã‚‰å›ç­”ã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ ```json ``` ã§å›²ã‚“ã§ãã ã•ã„:
{
  "claude_answer": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”ï¼ˆæ—¥æœ¬èªï¼‰",
  "local_llm_instructions": { ... },
  "complexity": "simple|moderate|complex",
  "skip_phase2": false
}"""
    else:  # p3
        return """ã‚ãªãŸã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®çµ±åˆãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã†AIã§ã™ã€‚
Phase 1ã®è¨ˆç”»ã¨Phase 2ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMå®Ÿè¡Œçµæœã‚’æ¯”è¼ƒãƒ»çµ±åˆã—ã€
æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ ```json ``` ã§å›²ã‚“ã§ãã ã•ã„:
{
  "status": "complete",
  "final_answer": "çµ±åˆã•ã‚ŒãŸæœ€çµ‚å›ç­”ï¼ˆæ—¥æœ¬èªï¼‰"
}"""


def _build_phase1_prompt(user_prompt: str) -> str:
    """Phase 1ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆMixAIOrchestratorã®_execute_phase1ç›¸å½“ï¼‰"""
    from src.utils.i18n import get_language
    _lang = get_language()
    _lang_line = "Respond in English." if _lang == "en" else "æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    _answer_example = "Your direct answer" if _lang == "en" else "ã‚ãªãŸã®ç›´æ¥å›ç­”"
    return f"""{_lang_line}
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®2ã¤ã‚’æä¾›ã—ã¦ãã ã•ã„:

1. ã‚ãªãŸè‡ªèº«ã®å›ç­” (claude_answer)
2. ãƒ­ãƒ¼ã‚«ãƒ«LLMã¸ã®æŒ‡ç¤º (local_llm_instructions)

JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:
{{
  "claude_answer": "{_answer_example}",
  "complexity": "low|medium|high",
  "skip_phase2": false,
  "local_llm_instructions": {{
    "coding": {{"skip": false, "prompt": "ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦³ç‚¹ã§ã®åˆ†ææŒ‡ç¤º", "expected_output": "ã‚³ãƒ¼ãƒ‰ä¾‹", "timeout_seconds": 300}},
    "research": {{"skip": false, "prompt": "èª¿æŸ»è¦³ç‚¹ã§ã®åˆ†ææŒ‡ç¤º", "expected_output": "èª¿æŸ»çµæœ", "timeout_seconds": 300}},
    "reasoning": {{"skip": false, "prompt": "æ¨è«–è¦³ç‚¹ã§ã®åˆ†ææŒ‡ç¤º", "expected_output": "æ¨è«–çµæœ", "timeout_seconds": 300}}
  }}
}}

complexity=lowã®å ´åˆã¯skip_phase2=trueã¨ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:
{user_prompt}"""


def _build_phase2_tasks(llm_instructions: dict, model_assignments: dict) -> list:
    """Phase 2ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆæ§‹ç¯‰"""
    tasks = []
    for category, spec in llm_instructions.items():
        if isinstance(spec, dict) and not spec.get("skip", True):
            model = model_assignments.get(category)
            if not model:
                continue
            prompt = spec.get("prompt", "").strip()
            if not prompt:
                continue
            tasks.append({
                "category": category,
                "model": model,
                "prompt": prompt,
                "timeout": spec.get("timeout_seconds", 300),
            })
    return tasks


def _build_phase3_prompt(user_prompt: str, claude_answer: str,
                         phase2_results: list) -> str:
    """Phase 3ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
    from src.utils.i18n import get_language
    _lang = get_language()
    _lang_line = "Respond in English." if _lang == "en" else "æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    _answer_example = "Integrated final answer" if _lang == "en" else "çµ±åˆã•ã‚ŒãŸæœ€çµ‚å›ç­”"
    results_text = ""
    for r in phase2_results:
        if r["success"]:
            results_text += f"\n### {r['category']} ({r['model']})\n{r['response'][:5000]}\n"

    return f"""{_lang_line}
ä»¥ä¸‹ã®æƒ…å ±ã‚’çµ±åˆã—ã¦ã€æœ€é«˜å“è³ªã®æœ€çµ‚å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{user_prompt}

## Phase 1 Claudeå›ç­”
{claude_answer[:8000]}

## Phase 2 ãƒ­ãƒ¼ã‚«ãƒ«LLMçµæœ
{results_text}

## æŒ‡ç¤º
å…¨ã¦ã®æƒ…å ±ã‚’çµ±åˆã—ã€æœ€çµ‚å›ç­”ã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:
{{"final_answer": "{_answer_example}"}}"""


# =============================================================================
# ä¼šè©±ä¿å­˜ãƒ˜ãƒ«ãƒ‘ãƒ¼
# =============================================================================

async def _save_web_conversation(messages: list, tab: str):
    """Webä¼šè©±ã‚’RAGã«éåŒæœŸä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ç„¡è¦–ï¼‰"""
    try:
        session_id = await rag_bridge.save_conversation(messages, tab)
        logger.info(f"Web conversation saved to RAG: {session_id}")
    except Exception as e:
        logger.warning(f"Conversation save failed: {e}")


# =============================================================================
# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ï¼‰
# =============================================================================

def start_server(host: str = "0.0.0.0", port: int = 8500):
    """Uvicornã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
    import uvicorn
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True,
    )


# =============================================================================
# PyQt6çµ±åˆ: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
# =============================================================================

import threading


class WebServerThread:
    """PyQt6ã‹ã‚‰èµ·å‹•ã™ã‚‹Web UIã‚µãƒ¼ãƒãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰"""

    def __init__(self, host="0.0.0.0", port=8500):
        self.host = host
        self.port = port
        self._thread = None
        self._server = None

    def start(self):
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()

    def _run(self):
        import uvicorn
        config = uvicorn.Config(app, host=self.host, port=self.port,
                                log_level="info")
        self._server = uvicorn.Server(config)
        self._server.run()

    def stop(self):
        if self._server:
            self._server.should_exit = True

    @property
    def is_running(self):
        return self._thread is not None and self._thread.is_alive()


def start_server_background(port=8500) -> WebServerThread:
    """PyQt6ã‹ã‚‰ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ï¼ˆåŒä¸€ãƒ—ãƒ­ã‚»ã‚¹ç‰ˆã€ç›´æ¥ä½¿ç”¨éæ¨å¥¨ï¼‰

    NOTE: PyQt6ã®è¨­å®šç”»é¢ã‹ã‚‰ã¯launcher.pyã®start_server_backgroundã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚
    ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’importã™ã‚‹ã¨fastapiã®ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«importãŒèµ°ã‚‹ãŸã‚ã€
    fastapiæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç’°å¢ƒã§ã¯ImportErrorãŒç™ºç”Ÿã™ã‚‹ã€‚
    """
    server = WebServerThread(port=port)
    server.start()
    return server


if __name__ == "__main__":
    import sys

    # HELIX_WEB_SERVER_ONLY=1 ã®å ´åˆã¯ã‚µãƒ¼ãƒãƒ¼ã®ã¿èµ·å‹•ï¼ˆPyQt6ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’é–‹ã‹ãªã„ï¼‰
    if os.environ.get("HELIX_WEB_SERVER_ONLY") != "1":
        print("Warning: HELIX_WEB_SERVER_ONLY is not set. "
              "If launched from PyQt6, set this env var to prevent GUI spawn.",
              file=sys.stderr)

    port = int(sys.argv[1]) if len(sys.argv) > 1 else 8500
    start_server(port=port)
